{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 200)\n",
      "(22626, 200)\n",
      "1000\n",
      "22626\n",
      "0\n",
      "[[-6.119545    9.335428    6.468402   ...  1.7206684  -4.347538\n",
      "  -3.5573971 ]\n",
      " [-3.2818186  -8.0842085  -1.8994448  ... -1.666454    4.0965376\n",
      "   2.3279297 ]\n",
      " [-2.9520621   0.49858949 -4.2866917  ...  1.158545   -1.1788055\n",
      "   0.98224372]\n",
      " ...\n",
      " [ 3.805325   -2.3143718   3.0308058  ...  1.8956968   4.5432563\n",
      "  -0.3395977 ]\n",
      " [-2.6695356   3.6270576   0.95769393 ...  2.5834267   1.1031551\n",
      "  -2.2075989 ]\n",
      " [-0.68940204 -0.28450504  0.48868689 ...  1.25368071 -0.60991341\n",
      "   2.68884778]]\n"
     ]
    }
   ],
   "source": [
    "FEATURES_PATH = \"Desktop/X_features.txt\"\n",
    "LABELS_PATH =\"Desktop/labels.txt\"\n",
    "\n",
    "TRAIN_SZ = 800\n",
    "TEST_SZ = 200\n",
    "\n",
    "# Change this for a different train size\n",
    "ACTUAL_TRAIN_SZ = 1000\n",
    "\n",
    "def load_labels():\n",
    "    labels = []\n",
    "    with open(LABELS_PATH, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        labels = [int(line) for line in lines]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_features ():\n",
    "    features = []\n",
    "    vec = []\n",
    "    append = False\n",
    "    with open(FEATURES_PATH, 'r') as f:\n",
    "        lines = f.readlines() \n",
    "        for line in lines:\n",
    "            if '[' in line:\n",
    "                i = line.index('[')\n",
    "                line = line[i+1:]\n",
    "\n",
    "            if ']' in line:\n",
    "                i = line.index(']')\n",
    "                line = line[:i]\n",
    "                \n",
    "                append = True\n",
    "\n",
    "            line = line.split()\n",
    "            nums = [float(elem) for elem in line]\n",
    "\n",
    "            vec = vec + nums\n",
    "            \n",
    "            if append == True > 0:\n",
    "                features.append(vec)\n",
    "                append = False\n",
    "                vec = []\n",
    "\n",
    "    return features\n",
    "\n",
    "def load_data_and_split():\n",
    "    # Load\n",
    "    labels = load_labels()\n",
    "    features = load_features()\n",
    "    # Split\n",
    "    train_labels = labels[:ACTUAL_TRAIN_SZ]\n",
    "    test_labels = labels[TRAIN_SZ:]\n",
    "\n",
    "    train_features = features[:ACTUAL_TRAIN_SZ]\n",
    "    test_features = features[TRAIN_SZ:]\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load the splitted data\n",
    "    train_feats, test_feats, train_lbls, test_lbls = load_data_and_split()\n",
    "\n",
    "    # Use np arrays for train/test or any other optimized format\n",
    "    train_feats = np.array(train_feats)\n",
    "    test_feats = np.array(test_feats)\n",
    "\n",
    "    print(train_feats.shape)\n",
    "    print(test_feats.shape)\n",
    "    \n",
    "    print(len(train_lbls))\n",
    "    print(len(test_lbls))\n",
    "    \n",
    "    print(train_lbls[0])\n",
    "    print(train_feats)\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = \"Desktop/X_features.txt\"\n",
    "LABELS_PATH =\"Desktop/labels.txt\"\n",
    "\n",
    "TRAIN_SZ = 800\n",
    "TEST_SZ = 200\n",
    "\n",
    "# Change this for a different train size\n",
    "ACTUAL_TRAIN_SZ = 1000\n",
    "\n",
    "def load_labels():\n",
    "    labels = []\n",
    "    with open(LABELS_PATH, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        labels = [int(line) for line in lines]\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_features ():\n",
    "    features = []\n",
    "    vec = []\n",
    "    append = False\n",
    "    with open(FEATURES_PATH, 'r') as f:\n",
    "        lines = f.readlines() \n",
    "        for line in lines:\n",
    "            if '[' in line:\n",
    "                i = line.index('[')\n",
    "                line = line[i+1:]\n",
    "\n",
    "            if ']' in line:\n",
    "                i = line.index(']')\n",
    "                line = line[:i]\n",
    "                \n",
    "                append = True\n",
    "\n",
    "            line = line.split()\n",
    "            nums = [float(elem) for elem in line]\n",
    "\n",
    "            vec = vec + nums\n",
    "            \n",
    "            if append == True > 0:\n",
    "                features.append(vec)\n",
    "                append = False\n",
    "                vec = []\n",
    "\n",
    "    return features\n",
    "\n",
    "def load_data_and_split():\n",
    "    # Load\n",
    "    labels = load_labels()\n",
    "    features = load_features()\n",
    "    # Split\n",
    "    train_labels = labels[:TRAIN_SZ]\n",
    "    test_labels = labels[TRAIN_SZ:ACTUAL_TRAIN_SZ]\n",
    "\n",
    "    train_features = features[:TRAIN_SZ]\n",
    "    test_features = features[TRAIN_SZ:ACTUAL_TRAIN_SZ]\n",
    "\n",
    "    return train_features, test_features, train_labels, test_labels\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 200)\n",
      "(200, 200)\n",
      "800\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Load the splitted data\n",
    "train_feats, test_feats, train_lbls, test_lbls = load_data_and_split()\n",
    "\n",
    "    # Use np arrays for train/test or any other optimized format\n",
    "train_feats = np.array(train_feats)\n",
    "test_feats = np.array(test_feats)\n",
    "\n",
    "print(train_feats.shape)\n",
    "print(test_feats.shape)\n",
    "    \n",
    "print(len(train_lbls))\n",
    "print(len(test_lbls))\n",
    "    \n",
    "train_df = DataFrame(data=train_feats)\n",
    "test_df = DataFrame(data=test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4          5          6    \\\n",
      "0 -6.119545  9.335428  6.468402  7.271471  3.748805  18.786093 -12.966435   \n",
      "1 -3.281819 -8.084209 -1.899445 -5.437369 -0.317577  -5.777606   4.005427   \n",
      "2 -2.952062  0.498589 -4.286692 -2.555032 -1.698925  -2.851132  -3.685697   \n",
      "3 -0.405626  1.372900 -0.606860 -2.083548  0.972744   5.510017  -1.138913   \n",
      "4 -0.539626  0.936998 -0.072238  2.834286 -0.823360   2.077538  -1.259922   \n",
      "\n",
      "        7         8         9      ...           190       191       192  \\\n",
      "0  5.858150  2.658198  2.744272    ...     10.152273 -1.414693  2.176323   \n",
      "1 -1.639933  2.290943  3.885992    ...     -2.290794 -3.975505  1.624542   \n",
      "2 -4.418259 -2.889379  2.056775    ...      3.992828  3.670976  7.565353   \n",
      "3 -0.996764  4.064074 -0.685127    ...      1.120871  4.334123 -1.043085   \n",
      "4 -4.703872  0.360579  2.477368    ...      0.884087  0.841968  0.601187   \n",
      "\n",
      "        193       194       195       196       197       198       199  \n",
      "0 -3.747780 -5.610521 -4.283277  7.296545  1.720668 -4.347538 -3.557397  \n",
      "1 -1.673590 -2.029048 -2.361072 -4.452268 -1.666454  4.096538  2.327930  \n",
      "2 -1.612651  1.949642  8.655140  0.360606  1.158545 -1.178805  0.982244  \n",
      "3 -0.677053 -1.904557  3.106980  3.621923 -2.649218  1.084511 -0.835074  \n",
      "4 -0.130403 -0.981989 -1.029575 -1.820137  1.747585  0.721514  2.093083  \n",
      "\n",
      "[5 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_np = np.array(train_lbls)\n",
    "test_labels_np = np.array(test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 0 1\n",
      " 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 1\n",
      " 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0\n",
      " 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 1 0\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 0\n",
      " 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 0 1 0 0 1 0\n",
      " 1 1 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0\n",
      " 0 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 0\n",
      " 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 1\n",
      " 0 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 1\n",
      " 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1\n",
      " 0 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1\n",
      " 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1]\n",
      "[0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_np)\n",
    "print(train_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb = BernoulliNB(binarize=0.0)\n",
    "bnb.fit(train_df,train_labels_np)\n",
    "bnb.score(test_df,test_labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = test_df\n",
    "bnb.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = bnb.predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "ans = 0\n",
    "for i in range(len(test_labels_np)):\n",
    "    if comp[i]==test_labels_np[i]:\n",
    "        ans+=1\n",
    "    \n",
    "print(ans/len(test_labels_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_df, train_labels_np)\n",
    "Y_gnb_score = gnb.predict_proba(test_df)\n",
    "lr = LogisticRegression() \n",
    "lr.fit(train_df,train_labels_np)\n",
    "Y_lr_score = lr.decision_function(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.38592718 -29.77975808   8.65664969 -51.6912235    9.619871\n",
      "  35.95523585  -6.49892209  11.14156582 -15.60778063  -1.54611166\n",
      " -36.25654148 -22.6335075   -7.50583338  11.99193256 -15.46214451\n",
      " -81.149422   -24.55144248   5.83545856   3.53001677  -9.24006243\n",
      "  -3.37295042   4.28680823  -9.10573948 -13.53016129  15.47085792\n",
      "  -4.71243418   5.82874987   5.92095773  15.25969856   5.70863963\n",
      "  -0.36102929  21.54202939  13.33634191  20.79579022  -0.40582143\n",
      "   3.28628412  13.20346149   2.49188795   9.72991677  -6.99048147\n",
      " -16.75819231  14.02792118   4.80371516   3.4399446   -7.07484031\n",
      " -15.38894512  27.46016834 -18.80822517   1.95390983 -27.53657836\n",
      "  14.74688714 -11.58792502  -1.79408545  10.23626136  11.52561161\n",
      " -14.27585246 -56.56398983  12.06108159  -9.53815536  -9.01935021\n",
      "  13.36568981  -1.23089162 -13.4208354    6.69885752  13.39823586\n",
      "  -3.42217287 -12.00034644   6.40689423 -15.4779878    2.92510541\n",
      " -33.33882389  12.57929214  -5.53110869  -4.8305958    8.69592544\n",
      "  93.02015677   2.95803372  21.60032143  -9.62025134 -19.8404919\n",
      "   9.28096804  -1.71691707 -19.64621088   9.21546709  10.62537167\n",
      "  18.69008528 -33.0578888  -18.73975374   4.47557906  21.46389868\n",
      " -21.59581826  27.60114309   3.94425403   6.88288507  10.69373185\n",
      "   6.54598157 -43.01360225 -10.31309986 -25.78029504  -6.01949781\n",
      "   0.17422644  13.29516821  40.57543233   3.18968726   9.75811005\n",
      "   5.45087636   9.40916719   7.22040922   5.56353685 -17.80996924\n",
      " -21.89317561 -13.48663842 -32.5801993   10.25330662  10.5981669\n",
      "   5.6580725   22.39010039 -17.89925702  24.09870096   1.88379399\n",
      "  -9.33493637   4.74068798  10.30206641   3.88907996  -4.31277717\n",
      "   7.3939044  -11.19344664   5.20480148  10.95462311   4.96314216\n",
      "   6.99941195  22.58575425  29.59360229   2.10581233 -29.81414354\n",
      "   2.51654057   2.47365039 -22.87797424  32.14569637   6.68031865\n",
      "  29.98570579 -12.51528671 -11.2104991   -1.87905739   4.11787793\n",
      " -47.05245008  -4.417568     2.35074028  31.13958607 -21.85559142\n",
      "   3.99357784   5.05755303  25.56376568  12.65347088  12.12409418\n",
      "  11.14081389  -3.43495681 -30.93947861   7.60148638  10.33123853\n",
      "  14.2454779  -10.75632843  -3.63317633  14.99454037  10.81751603\n",
      "  22.18335851  10.07320387  10.66555837  10.81947657  -4.81192599\n",
      "  12.87890681  33.12192195  11.83733796  17.86629438 -38.65521506\n",
      "   2.93009989   5.67488265  16.555713     5.54915772  -6.81072318\n",
      "  10.38362648 -14.58545411  -8.62426236 -77.95274841 -51.26373774\n",
      "  32.85124785  35.76825943  -2.61971399  18.22107349  12.05460948\n",
      "   7.31315796  -8.15034378 -23.09143878 105.35875717  12.98196151\n",
      "  11.86779803   3.68429432  -0.6086897   11.55167801   8.71978215]\n"
     ]
    }
   ],
   "source": [
    "print(Y_lr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_scores = test_labels_np \n",
    "y_predicted_scores = lr.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(y_true_scores))\n",
    "print(len(y_predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8785808280190301\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_true_scores, y_predicted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7687292936396312\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "print(cross_val_score(gnb, train_df, train_labels_np, scoring='accuracy', cv=10).mean())\n",
    "#print(cross_val_score(mnb, train_df, train_labels_np, scoring='accuracy', cv=10).mean())\n",
    "#Multinomial Naive baayes does not work because this algorithm can work just with positive values for its features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72  17]\n",
      " [  8 103]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.81      0.85        89\n",
      "           1       0.86      0.93      0.89       111\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       200\n",
      "   macro avg       0.88      0.87      0.87       200\n",
      "weighted avg       0.88      0.88      0.87       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(train_df, train_labels_np)\n",
    "y_pred_svm = svclassifier.predict(test_df)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels_np, y_pred_svm))\n",
    "print(classification_report(test_labels_np, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 61  28]\n",
      " [ 10 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.69      0.76        89\n",
      "           1       0.78      0.91      0.84       111\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       200\n",
      "   macro avg       0.82      0.80      0.80       200\n",
      "weighted avg       0.82      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='poly',gamma='auto')\n",
    "svclassifier.fit(train_df, train_labels_np)\n",
    "y_pred_svm = svclassifier.predict(test_df)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels_np, y_pred_svm))\n",
    "print(classification_report(test_labels_np, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86  3]\n",
      " [38 73]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.81        89\n",
      "           1       0.96      0.66      0.78       111\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       200\n",
      "   macro avg       0.83      0.81      0.79       200\n",
      "weighted avg       0.84      0.80      0.79       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='rbf',gamma='auto')\n",
    "svclassifier.fit(train_df, train_labels_np)\n",
    "y_pred_svm = svclassifier.predict(test_df)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels_np, y_pred_svm))\n",
    "print(classification_report(test_labels_np, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22 67]\n",
      " [63 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.25      0.25        89\n",
      "           1       0.42      0.43      0.42       111\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       200\n",
      "   macro avg       0.34      0.34      0.34       200\n",
      "weighted avg       0.35      0.35      0.35       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='sigmoid',gamma='auto')\n",
    "svclassifier.fit(train_df, train_labels_np)\n",
    "y_pred_svm = svclassifier.predict(test_df)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_labels_np, y_pred_svm))\n",
    "print(classification_report(test_labels_np, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe conclusion is that the best performance is obtained for Support vector machines with Radial Basis Function, because we have \\na high precision for malicious software and a low recall on non malicious files and that's a purpose ideal in this case\\nbecause we would not want to classify a non malicious file as a malicious one.\\n\\nI concluded with the following precisions for each algorithm specifically, for SVM we have two measurements for prediction of \\nMalicious(1) and Clear(0) values.\\n\\nNaive Bayes:\\n0.75 - Bernoulli Naive Bayes\\n0.76 - Gaussian Naive Bayes\\n\\nSVM:\\n0.42 and 0.26 -  SVM with Sigmoid Kernel\\n0.96 and 0.69-  SVM with RBF Kernel\\n0.78 and 0.86 -  SVM with Linear Kernel\\n0.86 and 0.90 -  SVM with Poly Kernel\\n\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The conclusion is that the best performance is obtained for Support vector machines with Radial Basis Function, because we have \n",
    "a high precision for malicious software and a low recall on non malicious files and that's a purpose ideal in this case\n",
    "because we would not want to classify a non malicious file as a malicious one.\n",
    "\n",
    "I concluded with the following precisions for each algorithm specifically, for SVM we have two measurements for prediction of \n",
    "Malicious(1) and Clear(0) values.\n",
    "\n",
    "Naive Bayes:\n",
    "0.75 - Bernoulli Naive Bayes\n",
    "0.76 - Gaussian Naive Bayes\n",
    "\n",
    "SVM:\n",
    "0.42 and 0.26 -  SVM with Sigmoid Kernel\n",
    "0.96 and 0.69-  SVM with RBF Kernel\n",
    "0.78 and 0.86 -  SVM with Linear Kernel\n",
    "0.86 and 0.90 -  SVM with Poly Kernel\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
